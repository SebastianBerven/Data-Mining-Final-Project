{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Crime Rate per County From Economic Factors\n",
    "#### CPSC 310 Data Mining  \n",
    "Gonzaga University  \n",
    "Sebastian Berven and Brett Barinaga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://fvmstatic.s3.amazonaws.com/maps/m/US-PPT-02-0005.png\" style=\"float: left;\" width=\"400\" height=\"400\">\n",
    "<img src=\"https://media.wnyc.org/i/800/0/c/85/1/AP_880772131965_JL9iuj1.jpg\" width=\"400\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward\n",
    "\n",
    "The goal of our project is to find effective predictors for determining crime rates using economic information like property values and rent as well as poverty statistics. To do that, we will initially be using the above information in order to predict different crime rates, but also plan on using other techniques like association rule mining in order to determine which factors play a larger role in influencing those rates. In addition, we will also be looking at more advanced ways to visualize the data in order to make our findings more intuitive and clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "All of the datasets are for the years 2014-2015 and are split by county, which allows for some level of uniformity among the data. They all come in CSV format. The rent dataset has 40,424 rows and 13 relevant attributes: County ID, State Name, County Name, City, Zip Code, Land Area, Water Area, Latitude, Longitude, Mean Rent, Median Rent, STDEV of rent, and sample size. The poverty dataset has 3,198 rows and 10 relevant attributes: the FIPS code, State, County Name, number of people in poverty, percentage of people in poverty, number of people over 17 in poverty, percentage of people over 17 in poverty, number of people ages 0-17 in poverty, percentage of people ages 0-17 in poverty, and median household income. The crime dataset has 3,136 rows and 12 attributes: County name, crime rate, number of murders, rapes, robberies, aggravated assaults, burglaries, larcenies, motor vehicle thefts, arsons, county population, and the FIPS code.\n",
    "\n",
    "Rent dataset: https://www.kaggle.com/goldenoakresearch/acs-gross-rent-us-statistics/version/3\n",
    "Poverty dataset: https://www.kaggle.com/rrp170330/variability-in-the-poverty-rate-in-the-us-counties#PovertyEstimates.xls\n",
    "Crime dataset: https://www.kaggle.com/mikejohnsonjr/united-states-crime-rates-by-county/version/1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Some preprocessing will be necessary, as there is some significant cleaning to be done in order to integrate multiple datasets and deal with conflicting and missing information. In addition, the size of the datasets will require significant computing power in order to obtain useful information. We believe these results will be useful as they will give some insight on the correlation between income, local costs, poverty, and their influence on local crime. This can lead to more effective law enforcement, as well as more efficient philanthropic efforts. These findings would be useful to entities like local and state governments in order to better target welfare programs and help in rent controlling efforts. They would also be useful for charity groups looking to combat poverty and law enforcement groups looking to better focus their limited manpower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier One: KNN\n",
    "Our first approach for classifying the data is using a k-Nearest Neighbor classifier. This involves an unseen instance in the dataset, computing the the distance of that instance to everyother instance using some sort of distance metric based on its non-class label attributes, in this case we use the Euclidean Distance equation:\n",
    "\n",
    "$$\\sqrt{\\sum_{i=1}^{n}(a_i - b_i)^{2}}$$\n",
    "\n",
    "From there, for an unseen instance, take the top K nearest neighbors and use a voting system (in our case simple majority) to classify that instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Classifier Two: Naive-Bayes\n",
    "The next approach for classifying the data is using a naive-bayes classifier. This method of classification relies on Bayes Theorem: \n",
    "\n",
    "$$P(H|X) = \\frac{P(X|H)P(H)}{P(X)}$$\n",
    "\n",
    "Where P(H|X) is the probability of event H occuring, given condition X.\n",
    "\n",
    "This classifier is called \"Naive\" because it relies on the assumption that the effect of one attribute on the class label is independent from other attributes. Given an unseen instance, the probability of each class label option is calculated based on the values of the other attributes, then the highest probability is used to classify the instance.\n",
    "\n",
    "\n",
    "\n",
    "For our datasets, we have many continous attributes, and the Bayes equation is used for categorical attributes. We can still use the Naive-Bayes algorithm with our data, with the caveat that we must use an alternate method for computing the probabilites, in this case, using the Gaussian function.\n",
    "$$g(x, \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$  \n",
    "\n",
    "Where the probability of a Value $v_k$ is\n",
    "$$P(v_k|C) = g(v_k, \\mu_C, \\sigma_C)$$\n",
    "And \n",
    "- Let μC be the mean of attribute k for instances labeled as C\n",
    "- Let σC be the standard deviation of attribute k for instances labeled as C\n",
    "- The probability $P(v_k|C)$ is defined as:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Three: Decision Tree\n",
    "The third approach used for classifying an unseen instance, is utilizing a decision tree. Unlike kNN and Naive Bayes which are \"instance-at-a-time\" classifiers, decisions trees are rule-based. This means that they build a set of general rules from a training set, and then use these rules to classify a new instance. \n",
    "\n",
    "Example of a decision tree: <img src=\"https://raw.githubusercontent.com/GonzagaCPSC310/U5-Decision-Trees/master/figures/iphone_decision_tree_example.png\" width=\"800\" height=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Four: Random Forest \n",
    "The fourth approach differs from the previous classifiers, in that a random forest is not a single classifier, but a collection of classifiers, known as 'weak' learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Classifier Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional sources\n",
    "\n",
    "https://ucr.fbi.gov/crime-in-the-u.s/2015/crime-in-the-u.s.-2015/resource-pages/crime-clock\n",
    "https://en.wikipedia.org/wiki/County_(United_States)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
