{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Crime Rate per County From Economic Factors\n",
    "#### CPSC 310 Data Mining  \n",
    "Gonzaga University  \n",
    "Sebastian Berven and Brett Barinaga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://fvmstatic.s3.amazonaws.com/maps/m/US-PPT-02-0005.png\" style=\"float: left;\" width=\"400\" height=\"400\">\n",
    "<img src=\"https://media.wnyc.org/i/800/0/c/85/1/AP_880772131965_JL9iuj1.jpg\" width=\"400\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward\n",
    "\n",
    "The goal of our project is to find effective predictors for determining crime rates using economic information like property values and rent as well as poverty statistics. To do that, we will initially be using the above information in order to predict different crime rates, but also plan on using other techniques like association rule mining in order to determine which factors play a larger role in influencing those rates. In addition, we will also be looking at more advanced ways to visualize the data in order to make our findings more intuitive and clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "All of the datasets are for the years 2014-2015 and are split by county, which allows for some level of uniformity among the data. They all come in CSV format. The rent dataset has 40,424 rows and 13 relevant attributes: County ID, State Name, County Name, City, Zip Code, Land Area, Water Area, Latitude, Longitude, Mean Rent, Median Rent, STDEV of rent, and sample size. The poverty dataset has 3,198 rows and 10 relevant attributes: the FIPS code, State, County Name, number of people in poverty, percentage of people in poverty, number of people over 17 in poverty, percentage of people over 17 in poverty, number of people ages 0-17 in poverty, percentage of people ages 0-17 in poverty, and median household income. The crime dataset has 3,136 rows and 12 attributes: County name, crime rate, number of murders, rapes, robberies, aggravated assaults, burglaries, larcenies, motor vehicle thefts, arsons, county population, and the FIPS code.\n",
    "\n",
    "Rent dataset: https://www.kaggle.com/goldenoakresearch/acs-gross-rent-us-statistics/version/3\n",
    "Poverty dataset: https://www.kaggle.com/rrp170330/variability-in-the-poverty-rate-in-the-us-counties#PovertyEstimates.xls\n",
    "Crime dataset: https://www.kaggle.com/mikejohnsonjr/united-states-crime-rates-by-county/version/1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Some preprocessing will be necessary, as there is some significant cleaning to be done in order to integrate multiple datasets and deal with conflicting and missing information. In addition, the size of the datasets will require significant computing power in order to obtain useful information. We believe these results will be useful as they will give some insight on the correlation between income, local costs, poverty, and their influence on local crime. This can lead to more effective law enforcement, as well as more efficient philanthropic efforts. These findings would be useful to entities like local and state governments in order to better target welfare programs and help in rent controlling efforts. They would also be useful for charity groups looking to combat poverty and law enforcement groups looking to better focus their limited manpower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier One: KNN\n",
    "Our first approach for classifying the data is using a k-Nearest Neighbor classifier. This involves an unseen instance in the dataset, computing the the distance of that instance to everyother instance using some sort of distance metric based on its non-class label attributes, in this case we use the Euclidean Distance equation:\n",
    "\n",
    "$$\\sqrt{\\sum_{i=1}^{n}(a_i - b_i)^{2}}$$\n",
    "\n",
    "From there, for an unseen instance, take the top K nearest neighbors and use a voting system (in our case simple majority) to classify that instance. To do this, we used several helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "def knn_classifier(table, predicted, predictors, test_nums, k = 5):\n",
    "    '''\n",
    "    Conducts KNN classifier tests.\n",
    "    Parameter table: The table to be tested.\n",
    "    Parameter predicted: Column to be estimated.\n",
    "    Parameter predictors: List of predictors used to estimate.\n",
    "    Parameter test_nums: Number of tests to be conducted.\n",
    "    Parameter k: Amount of nearest neighbors to test.\n",
    "    '''\n",
    "    random_table = utils.shuffle_table(table)\n",
    "    random_table = normalize_table(table, predictors)\n",
    "    groups = utils.strat_partition(random_table, predicted, test_nums) # Creates equally distributed partitions.\n",
    "    results = []\n",
    "    for x in range(test_nums):\n",
    "        test_set = groups[x]\n",
    "        train_set = []\n",
    "        for y in groups[:x] + groups[x+1:]: # Creates test set\n",
    "            for z in y:\n",
    "                train_set.append(z)\n",
    "        results += run_knn_tests(train_set, test_set, k, predicted, predictors)\n",
    "    return results\n",
    "\n",
    "def run_knn_tests(training_set, test_set, k, predicted, predictors):\n",
    "    '''\n",
    "    Runs KNN tests.\n",
    "    Parameter training_set: Training data.\n",
    "    Parameter test_set: Testing data.\n",
    "    Parameter k: Number of closest neighbors to consider.\n",
    "    Returns: Test case, guess, actual.\n",
    "    '''\n",
    "    closest = []\n",
    "    for item in test_set:\n",
    "        test = []\n",
    "        for row in training_set:\n",
    "            row.append(euc_distance([item[x] for x in predictors], [row[x] for x in predictors])) # Calculates distance\n",
    "\n",
    "        training_set.sort(key=operator.itemgetter(-1)) # Sorts on distance.\n",
    "        candidates = training_set[:k] # Finds k lowest distances.\n",
    "        choices, count = utils.get_frequencies(candidates, predicted)\n",
    "        print(candidates)\n",
    "        test.append(item)\n",
    "        test.append(choices[count.index(max(count))]) # Finds most common class\n",
    "        test.append(item[predicted])\n",
    "        closest.append(test)\n",
    "\n",
    "        for row in training_set: # Deletes distance row\n",
    "            del row[-1]\n",
    "\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "among others that can be found in my Github. Then, when running that against our dataset, we get the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.91\n",
      "KNN Classifier Prediction of Crime Rate\n",
      "=============  ======  =====  ========  ===========  ==========  =======  =================\n",
      "Crime Rate?      High    Low    Medium    Very High    Very Low    Total    Recognition (%)\n",
      "=============  ======  =====  ========  ===========  ==========  =======  =================\n",
      "High                0      2         1            0           6        9               0\n",
      "Low                 1     19         4            0         313      337               5.64\n",
      "Medium              1      6         0            0          59       66               0\n",
      "Very High           0      0         0            0           4        4               0\n",
      "Very Low            0     76        12            0        2334     2422              96.37\n",
      "=============  ======  =====  ========  ===========  ==========  =======  =================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import KNN_Functions as knn\n",
    "\n",
    "data, _ = utils.read_table(\"combined_data_normalized.csv\", True)\n",
    "class_index = 4\n",
    "predictors = [2,3,5,9]\n",
    "results = knn.knn_classifier(data, class_index, predictors, 5, 5) # Runs KNN Tests\n",
    "accuracy = utils.compute_accuracy(results)\n",
    "print(\"The accuracy is\", accuracy)\n",
    "utils.confusion_matrix(results, \"Crime Rate?\", \"KNN Classifier Prediction of Crime Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing different values for the amount of neighbors and tests:  \n",
    "Number of tests equal to 5, 10, 15, 20 and KNN equal to 5, 10, 15 ... 100\n",
    "    \n",
    "   \n",
    "The best accuracy was obtained with tests = 5 and KNN = 5 with 91% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Two: Naive-Bayes\n",
    "The next approach for classifying the data is using a naive-bayes classifier. This method of classification relies on Bayes Theorem: \n",
    "\n",
    "$$P(H|X) = \\frac{P(X|H)P(H)}{P(X)}$$\n",
    "\n",
    "Where P(H|X) is the probability of event H occuring, given condition X.\n",
    "\n",
    "This classifier is called \"Naive\" because it relies on the assumption that the effect of one attribute on the class label is independent from other attributes. Given an unseen instance, the probability of each class label option is calculated based on the values of the other attributes, then the highest probability is used to classify the instance.\n",
    "\n",
    "\n",
    "\n",
    "For our datasets, we have many continous attributes, and the Bayes equation is used for categorical attributes. We can still use the Naive-Bayes algorithm with our data, with the caveat that we must use an alternate method for computing the probabilites, in this case, using the Gaussian function.\n",
    "$$g(x, \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$  \n",
    "\n",
    "Where the probability of a Value $v_k$ is\n",
    "$$P(v_k|C) = g(v_k, \\mu_C, \\sigma_C)$$\n",
    "And \n",
    "- Let μC be the mean of attribute k for instances labeled as C\n",
    "- Let σC be the standard deviation of attribute k for instances labeled as C\n",
    "- The probability $P(v_k|C)$ is defined as:\n",
    "\n",
    "\n",
    "In our implementation of Naive Bayes, we used the following helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classifier(table, header, k, predicted, predictors, cont_attr = []):\n",
    "    '''\n",
    "    Reads a table and returns the specified column.\n",
    "    Parameter table: The table to be tested.\n",
    "    Parameter k: Rounds of tests to be conducted.\n",
    "    Parameter predicted: Column to be estimated.\n",
    "    Parameter predictors: List of predictors used to estimate.\n",
    "    Parameter cont_attr: List of which predictors are continuous.\n",
    "    Returns: Linear regression results, KNN results.\n",
    "    '''\n",
    "    random_table = utils.shuffle_table(table)\n",
    "    \n",
    "    groups = utils.strat_partition(random_table, predicted, k) # Creates equally distributed partitions.\n",
    "    results = [] # Initializes necessary variables.\n",
    "    for x in range(k):\n",
    "        test_set = groups[x]\n",
    "        train_set = []\n",
    "        for y in groups[:x] + groups[x+1:]: # Creates test set\n",
    "            for z in y:\n",
    "                train_set.append(z)\n",
    "            \n",
    "        bayes_table = create_bayes_table(train_set, header, predicted, predictors, cont_attr)\n",
    "        results2 = run_bayes_tests(bayes_table, header, test_set, predicted, predictors, cont_attr) # Conducts test\n",
    "        results += results2\n",
    "        \n",
    "    return results\n",
    "\n",
    "def run_bayes_tests(table, header, test_set, predicted, classifiers, cont_attr):\n",
    "    solved_set = []\n",
    "    for item in test_set:\n",
    "        buffer = []\n",
    "        buffer.append(item)\n",
    "        result = calculate_bayes(table, item, header, classifiers, cont_attr) # Finds estimated value.\n",
    "        buffer.append(result) # Includes guess for the test.\n",
    "        buffer.append(item[predicted]) # Includes actual value for test.\n",
    "        solved_set.append(buffer)\n",
    "    return solved_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "among others that can be found in my Github. Then, when running that against our dataset, we get the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.9\n",
      "Naive Bayes Classifier Prediction of Crime Rate\n",
      "=============  ======  =====  ========  ===========  ==========  =======  =================\n",
      "Crime Rate?      High    Low    Medium    Very High    Very Low    Total    Recognition (%)\n",
      "=============  ======  =====  ========  ===========  ==========  =======  =================\n",
      "High                0      2         0            0           7        9               0\n",
      "Low                 0     60         2            0         275      337              17.8\n",
      "Medium              0     15         1            0          50       66               1.52\n",
      "Very High           0      1         0            0           3        4               0\n",
      "Very Low            0     57        24            0        2341     2422              96.66\n",
      "=============  ======  =====  ========  ===========  ==========  =======  =================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Naive_Bayes_Functions as bayes\n",
    "\n",
    "data, header = utils.read_table(\"combined_data_normalized.csv\", True)\n",
    "class_index = 4\n",
    "predictors = [2,3,5,9]\n",
    "results = bayes.naive_bayes_classifier(data, header, 10, class_index, predictors, [2,3,5,9])\n",
    "accuracy = utils.compute_accuracy(results)\n",
    "print(\"The accuracy is\", accuracy)\n",
    "utils.confusion_matrix(results, \"Crime Rate?\", \"Naive Bayes Classifier Prediction of Crime Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Three: Decision Tree\n",
    "The third approach used for classifying an unseen instance, is utilizing a decision tree. Unlike kNN and Naive Bayes which are \"instance-at-a-time\" classifiers, decisions trees are rule-based. This means that they build a set of general rules from a training set, and then use these rules to classify a new instance. \n",
    "\n",
    "Example of a decision tree: <img src=\"https://raw.githubusercontent.com/GonzagaCPSC310/U5-Decision-Trees/master/figures/iphone_decision_tree_example.png\" width=\"800\" height=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Four: Random Forest \n",
    "The fourth approach differs from the previous classifiers, in that a random forest is not a single classifier, but a collection of classifiers, known as 'weak' learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Classifier Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Topic: Geomapping Data Visualization\n",
    "\n",
    "For out new topic, we explored mapping data directly onto to a map, to show relationships between crime data and geographical location. Initially our search for such a technique was unsuccesful, with many implementations requiring datasets with map information built in. Since our datasets did not have this, it took a while trying to figure out how to implement this topic. \n",
    "\n",
    "Eventually we found basemap, part of a matplotlib, which doesn't come standard when installing matplotlib. Because of this, coupled with the fact that we weren't using Anaconda, installing basemap was a pain. After hours of searching for whl files, tar files, binaries, and poorly documented readmes, we eventually installed the library and began using it. Not long into the process, we realized that we needed a \"shape\" (.shp) file to hold information on how to draw the county map. We eventually found such a file from a government website used for data visualization (link at the end of this notebook). With that, we were able to draw a blank map with county lines drawn on. \n",
    "\n",
    "To actually place our data onto the map, we decided to use a scatter plot, with each county's longitude and latitude as the x and y axis. We then used varying class labels for our color representation (red for high crime, yellow for medium crime etc). With these componenets, we were able to superimpose our scatterplot over the map, and use the following code to generate a map, using crime as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional sources\n",
    "\n",
    "https://ucr.fbi.gov/crime-in-the-u.s/2015/crime-in-the-u.s.-2015/resource-pages/crime-clock\n",
    "https://en.wikipedia.org/wiki/County_(United_States)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
